notes

This first cartpole implementation uses a q table which discretizes the states into buckets. It's committed.

Next I want to use a deep Q network to replace the q table with a NN that approximates the quality function 
	- PPO and soft actor critic outperofrm q-learning in high dimensional state spaces. 
		- look into LSTM or GRU networks 

	- Implemented with Claude. It uses 1.3GB of RAM and training slows the more episodes it does. Something is probably wrong. Should compare to the online example
		- It also doesn't really use the GPU. Claude says a model this size probably won't benefit from the GPU. 
			- pytorch example: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
			- https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras/blob/master/chapter9-drl/dqn-cartpole-9.6.1.py

		- Noticed that learning rate does not decay. TODO? 

		- My intuition is that training should not take this long. Its a tiny network with only so many knobs to turn. I could do it manually with a PID loop in less time. There must be massive inefficiency here. Just my gut feeling. 

		- TODO: load a model and just view the cartpole run with no epsilon. 
			- DONE! Now that I can see it, it's not balancing great. It only cares ifit makes it to the end of an episode. THe angle isnt great and the cart continuously moves to the side. 
				- a more complex reward function will probably help. It should be punished proportional to the pole angle and cart velocity. 
					- DONE! Its only on episode 76 already the avg score is 41.69 vs 33.95 previously
						- score is still calculated the same way

		- TODO: implement with a tensor graph? Can this be computed more efficiently? For example with gradient_tape? Remove the rendering too. It doesn't work and I don't think Claude can fix it

		- I am removing the rending and making the training happen only at the END of the episode, as opposed to every simulation step as before. 
			- IT gets higher rewards in less time but more episodes. I think this is good. 
				- The epsilon only decays when it trains so the epsilon stays high for much longer. I should increase the decay rate. 
					- for comparison: epsilon decay was .995 and we gpt avg score 23 by episode 185.
						- changing to .98: avg score 23 at episode 69!! 
							- this is way better, but maybe a foregone conclusion becacuse I took randomness out. 
								- Yeah, the avg score is actually going backwards now. 
						- changing to .99
							- its better but it still plateaus and decreases as epsilon gets low. Why? It should only move forward...
								- Lets go backwards. Changing epsilon back to .995 and training every step. I want to see Claude's optimized replay function broke something
									- OK it's learning. Claude didn't break anything. Its still weird though. Training goes nowhere until epsilon gets below like .05 and then it gets much much better. 
										- Is this expected because of the randomness? How can I tell what's actually best for learning? 
											- Lets make it so every 10th episode it runs with no epsilon and no training, just for eval purposes
				- Can optimize the replay function
					- reduce deque size? not yet
					- only call model.predict once
						- claude helped me optimize this. I'm just gonna trust that it works.

		- TODO: experiment by changing the optimizer.

		- TODO: try differen epsilon schemes. Maybe add a window to modify epsilon live. 
			- breakthroughs occured when epsilon was at .24 and .02. Lets first try modifyying the lower limit from .1
				- my initial random model did really really well this time.
					- its getting worse as it trains. 
						- had a gigantic breakthrough at episode 61. Will let it get to next eval 

		-TODO: more changes to reward function. Punished for displacement from center instead of cart velocity. DONE

		-TODO: add a function to save off a model at any time

		-TODO: track elapsed time DONE

		- I still don't understand why episodes take longer and longer to train. batch sizes don't grow even if the deque fills up. It seems like somethings wrong.
			- dequeue isnt filling up. Only at 3300 by episode 100. I will try reducing the max size and see if that helps speed or just hurts training. 
				- Yeah, I don't think it affects the training speed. The later episodes take longer because they last for more steps, which means it does more training iterations. The dequeue length probably affects memory usage but thats about it
					- Actually, I wonder if the earlier training data being in the deque means its training on old data. WOuld it do better if all the data came from later iterations with a better trained model? 
						- I should have all training runs start with the same untrained model for better comparison. DONE

		- I had claude do most of those TODO's. It's so good now! The evaluation run hit 495 on episode 100 after only 12 minutes of training!

		- It take approx 30 mins to beat the environment. My intuition is that this could be much faster. 
			- Claude suggested:
				- higher gamma
				- smaller batches
				- smaller replay buffer
				- Smaller network (on single layer (linear) with 8-16 neurons)
				- higher learning rate (.005 to .01)
				- much faster epsilon decay 
				- more frequent target network updates
				- simpler reward function (not gonna do this one)
				- PRIORITIZED REPLAY BUFFER
					- experiences with large differences between predicted Q and target Q are prioritized for batch selection
			- I will start by implementing smaller batches, replay buffer, higher learning rate, epsilon decay, min learning rate
				- none of that helped

		- The "last mile" of training isn't going well. It can't just stay balanced forever. I really wonder why
			- training also goes backwards sometimes. I had a model that scored a perfect 500 for 10 episodes in a row! then it regressed, getting scores around 250.
				- I think this is because the older data with worse angles/velocities fell out of the memory window
					- I will try increasing the window dramatically. No reason not to. 


- Good conversation with Claude about next steps
	- Need to move to a SAC (soft actor critic) algorith. Q-networks only work on discrete action spaces. SAC is state of the art for controls. Others include:
		- Variants like DrQ-v2 combine SAC with data augmentation for even better performance
		- Model-based methods (like MBPO, which incorporates SAC) can achieve even greater sample efficiency
		- Transformer-based architectures are beginning to show promise in certain control domains
	- Should parallelize gymnasium environments with AsyncVectorEnv. These will always run on CPU but I can multiprocess actoss 16 cores and generate 16x more diverse data 
	- Should use a more efficient memory architecture than Deque's. TF offers one: 
		class TFReplayBuffer:
	    def __init__(self, state_size, action_size, buffer_size=50_000):
	        # Preallocate tensors on CPU (faster transfers later)
	        self.states = tf.Variable(
	            tf.zeros((buffer_size, state_size), dtype=tf.float32),
	            trainable=False, name="replay_states")
	        self.actions = tf.Variable(
	            tf.zeros((buffer_size,), dtype=tf.int32),
	            trainable=False, name="replay_actions")
	        self.rewards = tf.Variable(
	            tf.zeros((buffer_size,), dtype=tf.float32),
	            trainable=False, name="replay_rewards")
	        self.next_states = tf.Variable(
	            tf.zeros((buffer_size, state_size), dtype=tf.float32),
	            trainable=False, name="replay_next_states")
	        self.dones = tf.Variable(
	            tf.zeros((buffer_size,), dtype=tf.bool),
	            trainable=False, name="replay_dones")
	        
	        self.buffer_size = buffer_size
	        self.current_size = 0
	        self.current_idx = 0
	    
	    def add(self, states_batch, actions_batch, rewards_batch, next_states_batch, dones_batch):
	        # Add batch of experiences (vectorized add for parallel environments)
	        batch_size = states_batch.shape[0]
	        
	        # Handle wrapping around buffer end
	        indices = tf.range(self.current_idx, self.current_idx + batch_size) % self.buffer_size
	        
	        # Use scatter updates to efficiently update buffer
	        self.states.scatter_update(tf.IndexedSlices(states_batch, indices))
	        self.actions.scatter_update(tf.IndexedSlices(actions_batch, indices))
	        self.rewards.scatter_update(tf.IndexedSlices(rewards_batch, indices))
	        self.next_states.scatter_update(tf.IndexedSlices(next_states_batch, indices))
	        self.dones.scatter_update(tf.IndexedSlices(dones_batch, indices))
	        
	        # Update tracking variables
	        self.current_idx = (self.current_idx + batch_size) % self.buffer_size
	        self.current_size = min(self.current_size + batch_size, self.buffer_size)
	    
	    def sample(self, batch_size):
	        # Sample a batch of experiences
	        indices = tf.random.uniform(
	            [batch_size], minval=0, maxval=self.current_size, dtype=tf.int32)
	        
	        # Gather sampled experiences - transfer to GPU happens here if needed
	        states = tf.gather(self.states[:self.current_size], indices)
	        actions = tf.gather(self.actions[:self.current_size], indices)
	        rewards = tf.gather(self.rewards[:self.current_size], indices)
	        next_states = tf.gather(self.next_states[:self.current_size], indices)
	        dones = tf.gather(self.dones[:self.current_size], indices)
	        
	        return states, actions, rewards, next_states, dones